@inproceedings{mccann2017learned,
  title={Learned in translation: Contextualized word vectors},
  author={McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  booktitle={Proceedings of NIPS},
  year={2017}
}
@InProceedings{N18-2084,
  author = 	"Qi, Ye
		and Sachan, Devendra
		and Felix, Matthieu
		and Padmanabhan, Sarguna
		and Neubig, Graham",
  title = 	"When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
  booktitle = 	"Proceedings of NAACL",
  year = 	"2018"
}
@InProceedings{N18-1202,
  author = 	"Peters, Matthew
		and Neumann, Mark
		and Iyyer, Mohit
		and Gardner, Matt
		and Clark, Christopher
		and Lee, Kenton
		and Zettlemoyer, Luke",
  title = 	"Deep Contextualized Word Representations",
  booktitle = 	"Proceedings of NAACL",
  year = 	"2018"
}
@InProceedings{P18-1031,
  author = 	"Howard, Jeremy
		and Ruder, Sebastian",
  title = 	"Universal Language Model Fine-tuning for Text Classification",
  booktitle = 	"Proceedings of ACL",
  year = 	"2018"
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={Technical report. OpenAI}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={Technical report. OpenAI}
}